{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8df47864",
   "metadata": {},
   "source": [
    "### Part 1: Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99d6a2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 01:59:33.668501: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-02 01:59:33.810275: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-02 01:59:33.810296: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-02 01:59:34.571902: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-02 01:59:34.571979: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-02 01:59:34.571987: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57f4cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "data = pd.read_json(\"../Sarcasm_Headlines_Dataset_v2.json\", lines=True, nrows=3000) #NOTE: Remove the nrows function to parse all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ae2b1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1452 sarcastic headlines and 1547 non-sarcastic headlines\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate headlines\n",
    "data=data.drop(data[data['headline'].duplicated()].index,axis=0)\n",
    "sarc_cnt = len(data.query('is_sarcastic==1'))\n",
    "non_sarc_cnt = len(data.query('is_sarcastic==0'))\n",
    "\n",
    "# Print out summary of sarcastic lines\n",
    "print(f'There are {sarc_cnt} sarcastic headlines and {non_sarc_cnt} non-sarcastic headlines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c001cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stopwords from nltk\n",
    "stwrds = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Define a method to clean a given headline by lowercasing the string, removing spaces, and removing stopwords\n",
    "def clean_headlines(headline):\n",
    "    headline = headline.lower()\n",
    "    headline_split = headline.split()\n",
    "    cleaned_headline = []\n",
    "    for word in headline_split:\n",
    "        if word not in stwrds and word not in string.punctuation:\n",
    "            cleaned_headline.append(word)\n",
    "    cleaned_line = \" \".join(cleaned_headline)\n",
    "    return cleaned_line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6563a85",
   "metadata": {},
   "source": [
    "### Part 2: Creating N-grams of Size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "389ae805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the headlines\n",
    "data['cleaned'] = data['headline'].apply(clean_headlines)\n",
    "\n",
    "# Create the N-grams (of size 2) for each headline\n",
    "cv = CountVectorizer(ngram_range=(2,2))\n",
    "res = cv.fit_transform(data.iloc[0:2].cleaned)\n",
    "res = cv.fit_transform(data.cleaned)\n",
    "\n",
    "features = pd.DataFrame(res.toarray(),columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae5ad1e",
   "metadata": {},
   "source": [
    "### Part 3: Creating and Training Gaussian Naive Bayes model using N-grams of size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5f8c2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test= train_test_split(features, data.is_sarcastic, test_size = 0.33)\n",
    "\n",
    "#Calling the Class\n",
    "naive_bayes = GaussianNB()\n",
    " \n",
    "#Fitting the data to the classifier\n",
    "naive_bayes.fit(X_train , y_train)\n",
    " \n",
    "#Predict on test data\n",
    "y_predicted = naive_bayes.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5c43f2",
   "metadata": {},
   "source": [
    "### Part 4: Display Model Metrics for N-grams of size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b51c2682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.93      0.70       528\n",
      "           1       0.70      0.18      0.29       462\n",
      "\n",
      "    accuracy                           0.58       990\n",
      "   macro avg       0.63      0.56      0.49       990\n",
      "weighted avg       0.63      0.58      0.51       990\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Give classification report\n",
    "\n",
    "print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c6c1fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the confusion matrix for the training dataset\n",
    "\n",
    "#confusion_matrix(y_train, naive_bayes.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a35c65e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[492,  36],\n",
       "       [379,  83]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Give the confusion matrix for the testing dataset\n",
    "\n",
    "confusion_matrix(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e244e3",
   "metadata": {},
   "source": [
    "### Part 5: Creating N-grams of Size 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8c7bc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the headlines\n",
    "data['cleaned'] = data['headline'].apply(clean_headlines)\n",
    "\n",
    "# Create the N-grams (of size 2) for each headline\n",
    "cv = CountVectorizer(ngram_range=(3,3))\n",
    "res = cv.fit_transform(data.iloc[0:2].cleaned)\n",
    "res = cv.fit_transform(data.cleaned)\n",
    "\n",
    "features = pd.DataFrame(res.toarray(),columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0f3b6b",
   "metadata": {},
   "source": [
    "### Part 6: Creating and Training Gaussian Naive Bayes model using N-grams of size 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49879087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test= train_test_split(features, data.is_sarcastic, test_size = 0.33)\n",
    "\n",
    "#Calling the Class\n",
    "naive_bayes = GaussianNB()\n",
    " \n",
    "#Fitting the data to the classifier\n",
    "naive_bayes.fit(X_train , y_train)\n",
    " \n",
    "#Predict on test data\n",
    "y_predicted = naive_bayes.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385f95ae",
   "metadata": {},
   "source": [
    "### Part 7: Display Model Metrics for N-grams of size 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "831791f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      1.00      0.70       531\n",
      "           1       1.00      0.01      0.03       459\n",
      "\n",
      "    accuracy                           0.54       990\n",
      "   macro avg       0.77      0.51      0.36       990\n",
      "weighted avg       0.75      0.54      0.39       990\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Give classification report\n",
    "\n",
    "print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c61e3640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the confusion matrix for the training dataset\n",
    "\n",
    "#confusion_matrix(y_train, naive_bayes.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3f6f402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[531,   0],\n",
       "       [453,   6]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Give the confusion matrix for the testing dataset\n",
    "\n",
    "confusion_matrix(y_test, y_predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
